<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="MARJu3erA7Z6jybcmR5fgSNRNtCNYMAYkk0jidGGr8A"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>GPT-2 | Nidhi Vakil</title> <meta name="author" content="Nidhi Vakil"> <meta name="description" content="Language Models are Unsupervised Multitask Learners"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://nidhivakil.github.io/2019/02/14/GPT-2.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.highlight pre:not(.language-text){background-color:#272822;color:#f8f8f2}.highlight .hll{background-color:#272822}.highlight .comment{color:#75715e}.highlight .err{color:#960050;background-color:#1e0010}.highlight .keyword{color:#66d9ef}.highlight .l{color:#ae81ff}.highlight .n{color:#f8f8f2}.highlight .operator{color:#f92672}.highlight .punctuation{color:#f8f8f2}.highlight .cm{color:#75715e}.highlight .cp{color:#75715e}.highlight .c1{color:#75715e}.highlight .cs{color:#75715e}.highlight .ge{font-style:italic}.highlight .gs{font-weight:bold}.highlight .kc{color:#66d9ef}.highlight .kd{color:#66d9ef}.highlight .kn{color:#f92672}.highlight .kp{color:#66d9ef}.highlight .kr{color:#66d9ef}.highlight .kt{color:#66d9ef}.highlight .ld{color:#e6db74}.highlight .number{color:#ae81ff}.highlight .string{color:#e6db74}.highlight .na{color:#a6e22e}.highlight .builtin{color:#f8f8f2}.highlight .class-name{color:#a6e22e}.highlight .no{color:#66d9ef}.highlight .decorator{color:#a6e22e}.highlight .ni{color:#f8f8f2}.highlight .ne{color:#a6e22e}.highlight .function{color:#a6e22e}.highlight .nl{color:#f8f8f2}.highlight .nn{color:#f8f8f2}.highlight .nx{color:#a6e22e}.highlight .py{color:#f8f8f2}.highlight .nt{color:#f92672}.highlight .nv{color:#f8f8f2}.highlight .ow{color:#f92672}.highlight .w{color:#f8f8f2}.highlight .mf{color:#ae81ff}.highlight .mh{color:#ae81ff}.highlight .mi{color:#ae81ff}.highlight .mo{color:#ae81ff}.highlight .sb{color:#e6db74}.highlight .sc{color:#e6db74}.highlight .sd{color:#e6db74}.highlight .s2{color:#e6db74}.highlight .se{color:#ae81ff}.highlight .sh{color:#e6db74}.highlight .si{color:#e6db74}.highlight .sx{color:#e6db74}.highlight .sr{color:#e6db74}.highlight .s1{color:#e6db74}.highlight .ss{color:#e6db74}.highlight .bp{color:#f8f8f2}.highlight .vc{color:#f8f8f2}.highlight .vg{color:#f8f8f2}.highlight .vi{color:#f8f8f2}.highlight .il{color:#ae81ff}.highlight .gu{color:#75715e}.highlight .gd{color:#f92672}.highlight .gi{color:#a6e22e}</style> <script>function createButton(t,o){const n=document.querySelector("body");backToTopButton=document.createElement("span"),backToTopButton.classList.add("softr-back-to-top-button"),backToTopButton.id="softr-back-to-top-button",o?o.appendChild(backToTopButton):n.appendChild(backToTopButton),backToTopButton.style.width=t.buttonWidth,backToTopButton.style.height=t.buttonHeight,backToTopButton.style.marginRight=t.buttonDToRight,backToTopButton.style.marginBottom=t.buttonDToBottom,backToTopButton.style.borderRadius=t.roundnessSize,backToTopButton.style.boxShadow=t.shadowSize,backToTopButton.style.color=t.selectedBackgroundColor,backToTopButton.style.backgroundColor=t.selectedBackgroundColor,backToTopButton.style.position=o?"absolute":"fixed",backToTopButton.style.outline="none",backToTopButton.style.bottom="0px",backToTopButton.style.right="0px",backToTopButton.style.cursor="pointer",backToTopButton.style.textAlign="center",backToTopButton.style.border="solid 2px currentColor",backToTopButton.innerHTML='<svg class="back-to-top-button-svg" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" > <g fill="none" fill-rule="evenodd"> <path d="M0 0H32V32H0z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> <path class="back-to-top-button-img" fill-rule="nonzero" d="M11.384 13.333h9.232c.638 0 .958.68.505 1.079l-4.613 4.07c-.28.246-.736.246-1.016 0l-4.613-4.07c-.453-.399-.133-1.079.505-1.079z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> </g> </svg>',backToTopButtonSvg=document.querySelector(".back-to-top-button-svg"),backToTopButtonSvg.style.verticalAlign="middle",backToTopButtonSvg.style.margin="auto",backToTopButtonSvg.style.justifyContent="center",backToTopButtonSvg.style.width=t.svgWidth,backToTopButtonSvg.style.height=t.svgHeight,backToTopButton.appendChild(backToTopButtonSvg),backToTopButtonImg=document.querySelector(".back-to-top-button-img"),backToTopButtonImg.style.fill=t.selectedIconColor,backToTopButtonSvg.appendChild(backToTopButtonImg),backToTopButtonImg.setAttribute("d",t.buttonD),backToTopButtonImg.setAttribute("transform",t.buttonT),o||(backToTopButton.style.display="none",window.onscroll=function(){document.body.scrollTop>20||document.documentElement.scrollTop>20?backToTopButton.style.display="block":backToTopButton.style.display="none"},backToTopButton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0})}configObj={buttonD:"M8 18.568L10.8 21.333 16 16.198 21.2 21.333 24 18.568 16 10.667z",buttonT:"translate(-1148 -172) translate(832 140) translate(32 32) translate(284)",shadowSize:"none",roundnessSize:"999px",buttonDToBottom:"64px",buttonDToRight:"32px",selectedBackgroundColor:"#c2c0bf",selectedIconColor:"#a31f34",buttonWidth:"40px",buttonHeight:"40px",svgWidth:"32px",svgHeight:"32px"},document.addEventListener("DOMContentLoaded",function(){createButton(configObj,null)});</script> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">NidhiÂ </span>Vakil</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>GPT-2</h1> <p>Language Models are Unsupervised Multitask Learners</p> </d-title> <d-byline> <div class="byline grid"> <div> <h3>Published</h3> <p>February 14, 2019</p> </div> <div> <h3>Paper</h3> <p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="external nofollow noopener" target="_blank">PDF</a></p> </div> <div> <h3>Code</h3> <p><a href="https://github.com/openai/gpt-2/" rel="external nofollow noopener" target="_blank">Github</a></p> </div> </div> </d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#takeaways">Takeaways</a></div> <div><a href="#introduction">Introduction</a></div> <div><a href="#methods">Methods</a></div> <ul> <li><a href="#language-model">Language Model</a></li> <li><a href="#training-dataset">Training Dataset</a></li> <li><a href="#input-representation">Input Representation</a></li> <li><a href="#model">Model</a></li> </ul> <div><a href="#experiments">Experiments</a></div> </nav> </d-contents> <h2 id="takeaways">Takeaways</h2> <ul> <li>GPT-2 begins to learn different tasks without any explicit supervision (<strong>zero-shot transfer</strong>) when trained on a new dataset of millions of webpages (WebText).</li> <li>GPT-2 is able to perform new tasks by conditioning on text that specifies the task and the input (<strong>prompt learning</strong>).</li> <li>The capacity of the LM is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.</li> </ul> <h2 id="introduction">Introduction</h2> <p>Supervised learning systems are brittle and sensitive to changes in distribution and task (ânarrow expertsâ). The prevalence of single-task training on single-domain datasets might be a major contributor to the lack of generalization observed in current systems.</p> <p>Multitask learning is a promising framework for improving general performance. However, multi-task training in NLP is still nascent.</p> <ul> <li>Each (dataset, objective) pair is considered as a sample.</li> <li>Current ML systems need hundreds to thousands of examples to induce functions that generalize well.</li> <li>Difficult to scale with current approaches.</li> </ul> <p>The trend of pre-trained language representation NLP:</p> <ol> <li>single-layer pre-trained word embedding + task-specific architectures</li> <li>multiple layers of representations (e.g. RNN) + task-specific architectures</li> <li>pre-train RNNs or Transformers, and then directly fine-tune, without task-specific architectures (e.g. GPT<d-cite key="GPT"></d-cite>, BERT<d-cite key="BERT"></d-cite>).</li> </ol> <p>This work: LM can perform a wide range of downstream tasks in a <strong>zero-shot</strong> setting, without any parameter or architecture modification.</p> <h2 id="methods">Methods</h2> <h3 id="language-model">Language Model</h3> <p>Training LMs in a probabilistic framework as estimating a conditional distribution of the output given the input and the task information (multi-task/meta-learning),</p> \[p(\texttt{output}|\texttt{input}, \texttt{task})\] <p>The language model is <em>auto-regressive</em>, i.e., it predicts the next word given the previous words.</p> <h4 id="task-conditioning">Task Conditioning</h4> <ul> <li>architectural level, e.g., task-specific encoders and decoders</li> <li>algorithmic level, e.g., the inner and outer loop optimization framework of MAML</li> <li> <strong>natural language</strong> provides a flexible way to specify tasks (prompt)</li> </ul> <h4 id="speculation">Speculation</h4> <p>LMs with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. If LMs are able to do this it will be, in effect, performing unsupervised multitask learning.</p> <p>Test this by analyzing the performance of LMs in a zero-shot setting on a wide variety of tasks.</p> <h3 id="training-dataset">Training Dataset</h3> <p>Although web scrapes such as Common Crawl are many orders of magnitude larger than current language modeling datasets, they have significant data quality issues.</p> <p>The authors created a new web scrape that emphasizes document quality.</p> <ol> <li>Only scraped web pages that have been curated/filtered by humans: scraped all outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.</li> <li>Extract the text from HTML responses</li> <li>Ee-duplication and some heuristic-based cleaning</li> <li>Removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.</li> </ol> <p>Results in over 8 million documents for a total of 40 GB of text (about 40 Billion Bytes).</p> <h3 id="input-representation">Input Representation</h3> <p>A general LM should be able to compute the probability of (and also generate) any string.</p> <p>While processing Unicode strings as a sequence of UTF-8 bytes elegantly fulfills this requirement, current byte-level LMs are not competitive with word-level LMs on large-scale datasets.</p> <h4 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h4> <p>An interpolation between word-level inputs for frequent symbol sequences and character-level inputs for infrequent symbol sequences.</p> <p>BPE on Unicode code points: The size of the base vocabulary is too large (&gt; 130,000) compared to the 32,000 to 64,000 token vocabularies often used with BPE.</p> <h4 id="bpe-on-byte-level">BPE on Byte Level</h4> <ol> <li>A base vocabulary of size 256</li> <li>Naive BPE results in suboptimal merges due to the greedy strategy. To avoid this, the authors prevent BPE from merging across character categories, with an exception for spaces.</li> <li>Enable the model to assign a probability to any Unicode string.</li> </ol> <h3 id="model">Model</h3> <p>The model largely follows the details of the GPT<d-cite key="GPT"></d-cite> model with a few modifications.</p> <ol> <li>LayerNorm was moved to the input of each sub-block and an additional LayerNorm was added after the final self-attention block.</li> <li>A modified initialization that accounts for the accumulation on the residual path with model depth is used. Scale the weights of residual layers at initialization by a factor of \(1/\sqrt{N}\), where \(N\) is the number of residual layers.</li> <li>The vocabulary is expanded to 50,257.</li> <li>We also increase the context size from 512 to 1024 tokens,</li> <li>A larger batch size of 512 is used.</li> </ol> <p>The largest model (GPT-2) has 1.5B parameters.</p> <h2 id="experiments">Experiments</h2> <h3 id="language-modeling">Language Modeling</h3> <p>This is the primary task the models are trained for.</p> <p>Task: Evaluating the log probability of different datasets according to the LM.</p> <p>Results: GPT-2 transfers well across domains and datasets, improving the state of the art on 7 out of the 8 datasets in a zero-shot setting.</p> <h3 id="lambada">LAMBADA</h3> <p>The LAMBADA dataset tests the ability of systems to model long-range dependencies in text. The</p> <p>Task: predict the final word of sentences that require at least 50 tokens of context for a human to successfully predict.</p> <p>Results: GPT-2 improves the SOTA.</p> <p>Common error: valid continuations of the sentence, but not valid final words.</p> <p>This suggests that the LM is not using the additional useful constraint that the word must be the final of the sentence. Adding a stop-word filter as an approximation to this further increases accuracy.</p> <h3 id="reading-comprehension">Reading Comprehension</h3> <p>The Conversation Question Answering dataset (CoQA) consists of documents from 7 different domains paired with natural language dialogues between a question asker and a question answerer about the document. CoQA tests reading comprehension capabilities and also the ability of models to answer questions that depend on conversation history (such as âWhy?â).</p> <p>Use greedy decoding from GPT-2 conditioned on a document, the history of the associated conversation, and a final token <code class="language-plaintext highlighter-rouge">A:</code>.</p> <p>Results: GPT-2 matches or exceeds the performance of 3 out of 4 baseline systems, and underperforms the supervised SOTA (BERT-based)</p> <h3 id="summarization">Summarization</h3> <p>Induce summarization behavior by adding the text <code class="language-plaintext highlighter-rouge">TL;DR:</code> after the article and generating 100 tokens with Top-\(k\) random sampling with \(k = 2\) which reduces repetition and encourages more abstractive summaries than greedy decoding. The first 3 generated sentences in these 100 tokens are used as the summary.</p> <p>Results:</p> <ul> <li>GPT-2 often focuses on recent content from the article or confuses specific details.</li> <li>GPT-2 only begins to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article.</li> </ul> <h3 id="translation">Translation</h3> <p>Help GPT-2 infer the translation task, by conditioning the LM on a context of example pairs of the format <code class="language-plaintext highlighter-rouge">english sentence = french sentence</code> followed by a final prompt of <code class="language-plaintext highlighter-rouge">english sentence =</code>. After the prompt, outputs are sampled with greedy decoding and the first generated sentence is used as the translation.</p> <p>Results:</p> <ul> <li>English-French: GPT-2 is slightly worse than a word-by-word substitution with a bilingual lexicon.</li> <li>French-English: GPT-2 is able to leverage its very strong English LM and outperforms several unsupervised baselines but is still much worse than the SOTA unsupervised approach.</li> </ul> <p>Note: Since non-English webpages were filtered from WebText, it only contains a very small (10MB) corpus in the Frech language.</p> <h3 id="question-answering">Question Answering</h3> <p>Similar to translation, the context of the language model is seeded with example question answer pairs which helps the model infer the short answer style of the dataset.</p> <p>Results: The performance of GPT-2 is still much, much, worse than the existing open domain question answering systems.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/paper-notes.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 Nidhi Vakil. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-NYJ88YK0VS"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-NYJ88YK0VS");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>